{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-general",
   "metadata": {},
   "source": [
    "# Myers-Briggs Personality Type Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-yacht",
   "metadata": {},
   "source": [
    "# 1 - Packages #\n",
    "\n",
    "Let's first import all the packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-compound",
   "metadata": {},
   "source": [
    "# 2 - Preprocessing helper functions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "copyrighted-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "from nltk.corpus import words\n",
    "\n",
    "# English Lexicon\n",
    "lexicon = {}\n",
    "for word in words.words():\n",
    "    lexicon[word] = True\n",
    "    \n",
    "# Data store\n",
    "data = []\n",
    "\n",
    "# Lexicon\n",
    "words_dict = {}\n",
    "word_val = 0\n",
    "\n",
    "# Personity types \n",
    "personality_type_dict = {}\n",
    "personality_type_val = 0\n",
    "\n",
    "# Snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# Removes url, punctuation, and digits\n",
    "def post_clean_up(post):\n",
    "    remove_url = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "    remove_punc = remove_url.translate(str.maketrans('', '', string.punctuation))\n",
    "    remove_digit = re.sub(r'\\d+', '', remove_punc)\n",
    "    remove_digit = remove_digit.strip()\n",
    "    return remove_digit\n",
    "\n",
    "# Applies snow ball stemmer and inserts root word to words_dict\n",
    "def apply_snow_ball_stemmer(post):\n",
    "    global word_val\n",
    "    processed_post = \"\"\n",
    "    for word in post.split():\n",
    "        if word not in lexicon:\n",
    "            continue\n",
    "        root_word = stemmer.stem(word)\n",
    "\n",
    "        if root_word not in words_dict:\n",
    "            words_dict[root_word] = word_val\n",
    "            word_val += 1\n",
    "\n",
    "        processed_post += \" \"+ root_word\n",
    "\n",
    "    processed_post = processed_post.strip()\n",
    "    return processed_post\n",
    "\n",
    "# Read file\n",
    "def read_file(location):\n",
    "    file = open(location, 'r')\n",
    "    return file.readlines()[1:]\n",
    "\n",
    "# Read json\n",
    "def read_json(location):\n",
    "    with open(location) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "# Save CSV\n",
    "def save_csv(data, resource_location):\n",
    "    np.savetxt(resource_location, data, fmt='%s, %s')\n",
    "\n",
    "# Save JSON\n",
    "def save_json(data, resource_location):\n",
    "    with open(resource_location, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-burton",
   "metadata": {},
   "source": [
    "# 3 - Preprocessing mbti dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compact-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(381453, 2) 106.30440447446999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open mbti file\n",
    "mbti_resource_location = \"../data/mbti_1.csv\"\n",
    "\n",
    "lines = read_file(mbti_resource_location)\n",
    "\n",
    "mean_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    personality_type, _, posts = line.partition(\",\")\n",
    "    \n",
    "    if personality_type not in personality_type_dict:\n",
    "        personality_type_dict[personality_type] = personality_type_val\n",
    "        personality_type_val += 1\n",
    "    \n",
    "    for post in posts.split(\"|||\"):\n",
    "        # Removing URLs, punctuation, and digits\n",
    "        post = post_clean_up(post)\n",
    "        \n",
    "        # Filter out posts with length less than 10\n",
    "        if len(post) > 10:\n",
    "            \n",
    "            # Apply Snowball stemmer\n",
    "            post = apply_snow_ball_stemmer(post)\n",
    "            \n",
    "            \n",
    "            # Filter out posts with length less than 10\n",
    "            if len(post) > 10:\n",
    "                mean_length += len(post)\n",
    "                example = [post, personality_type]\n",
    "                data.append(example)\n",
    "\n",
    "        \n",
    "            \n",
    "data = np.asarray(data)\n",
    "print(data.shape, mean_length/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exterior-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "processed_mbti_resource_location = \"../data/processed_mbti.csv\"\n",
    "words_dict_resource_location = '../data/words_dict.json'\n",
    "personality_type_dict_resource_location = '../data/personality_dict.json'\n",
    "\n",
    "\n",
    "# Save processed mbti data\n",
    "save_csv(data, processed_mbti_resource_location)\n",
    "# Save words_dict\n",
    "save_json(words_dict, words_dict_resource_location)\n",
    "# Save words_dict\n",
    "save_json(personality_type_dict, personality_type_dict_resource_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-string",
   "metadata": {},
   "source": [
    "# 4 - Training a deep neural network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy import sqrt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "processed_mbti_resource_location = \"../data/processed_mbti.csv\"\n",
    "words_dict_resource_location = '../data/words_dict.json'\n",
    "personality_type_dict_resource_location = '../data/personality_dict.json'\n",
    "\n",
    "\n",
    "processed_mbti = read_file(processed_mbti_resource_location)\n",
    "processed_mbti = processed_mbti[:len(processed_mbti)-10000]\n",
    "lexicon_dict = read_json(words_dict_resource_location)\n",
    "\n",
    "random.shuffle(processed_mbti)\n",
    "# Feature vector size\n",
    "n_features = 20404\n",
    "model  = Sequential()\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\", input_shape=(n_features,)))\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(20, activation='relu', kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "i = 0\n",
    "for r in range(0,len(processed_mbti), 500):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in processed_mbti[r:min(r+500, len(processed_mbti))]:\n",
    "        post, _, personality_type = line.partition(\",\")\n",
    "        feature = [0]*n_features\n",
    "\n",
    "        for word in post.split(\" \"):\n",
    "            if word in lexicon_dict:\n",
    "                feature[lexicon_dict[word]] = 1\n",
    "\n",
    "        X.append(feature)\n",
    "        Y.append(personality_type)\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    Y = LabelEncoder().fit_transform(Y)    \n",
    "    if i % 5 == 0:\n",
    "        print(\"############################################\")\n",
    "        model.fit(X, Y, epochs=15, batch_size=64, verbose=2)\n",
    "    else:\n",
    "        model.fit(X, Y, epochs=15, batch_size=64, verbose=0)\n",
    "    i += 1;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-traffic",
   "metadata": {},
   "source": [
    "# 5 - Saving the trained model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "improving-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-elder",
   "metadata": {},
   "source": [
    "# 6 - Testing the trained model on dev set #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "separated-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 - 0s - loss: 13.7735 - accuracy: 0.0905\n",
      "Test accuracy: 0.090\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "processed_mbti_resource_location = \"../data/processed_mbti.csv\"\n",
    "words_dict_resource_location = '../data/words_dict.json'\n",
    "personality_type_dict_resource_location = '../data/personality_dict.json'\n",
    "\n",
    "\n",
    "processed_mbti = read_file(processed_mbti_resource_location)\n",
    "processed_mbti = processed_mbti[len(processed_mbti)-10000:]\n",
    "lexicon_dict = read_json(words_dict_resource_location)\n",
    "\n",
    "\n",
    "random.shuffle(processed_mbti)\n",
    "processed_mbti = processed_mbti[:2000]\n",
    "\n",
    "# Feature vector size\n",
    "size = 20403\n",
    "\n",
    "X_dev = []\n",
    "Y_dev = []\n",
    "\n",
    "for line in processed_mbti:\n",
    "    post, _, personality_type = line.partition(\",\")\n",
    "    feature = [0]*size\n",
    "\n",
    "    for word in post.split(\" \"):\n",
    "        if word in lexicon_dict:\n",
    "            feature[lexicon_dict[word]] = 1\n",
    "            \n",
    "    X_dev.append(feature)\n",
    "    Y_dev.append(personality_type)\n",
    "X_dev = np.asarray(X_dev)\n",
    "Y_dev = np.asarray(Y_dev)\n",
    "\n",
    "\n",
    "model = load_model('model.h5')\n",
    "\n",
    "Y_dev = LabelEncoder().fit_transform(Y_dev)\n",
    "loss, acc = model.evaluate(X_dev, Y_dev, verbose=2)\n",
    "print(\"Test accuracy: %.3f\" % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
