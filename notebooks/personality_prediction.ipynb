{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "respected-general",
   "metadata": {},
   "source": [
    "# Myers-Briggs Personality Type Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-yacht",
   "metadata": {},
   "source": [
    "# 1 - Packages #\n",
    "\n",
    "Let's first import all the packages that you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-providence",
   "metadata": {},
   "source": [
    "# 2 - Dataset #\n",
    "\n",
    "Let's get the dataset we will work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-compound",
   "metadata": {},
   "source": [
    "## 2.1 - Preprocessing helper functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import words\n",
    "\n",
    "# English Lexicon\n",
    "lexicon = {}\n",
    "for word in words.words():\n",
    "    lexicon[word] = True\n",
    "    \n",
    "# Data store\n",
    "data = []\n",
    "\n",
    "# Lexicon\n",
    "words_dict = {}\n",
    "word_val = 0\n",
    "\n",
    "# Personity types \n",
    "personality_type_dict = {}\n",
    "personality_type_val = 0\n",
    "\n",
    "# Snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "# Removes url, punctuation, and digits\n",
    "def post_clean_up(post):\n",
    "    remove_url = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', post)\n",
    "    remove_punc = remove_url.translate(str.maketrans('', '', string.punctuation))\n",
    "    remove_digit = re.sub(r'\\d+', '', remove_punc)\n",
    "    remove_digit = remove_digit.strip()\n",
    "    return remove_digit\n",
    "\n",
    "# Applies snow ball stemmer and inserts root word to words_dict\n",
    "def apply_snow_ball_stemmer(post):\n",
    "    global word_val\n",
    "    processed_post = \"\"\n",
    "    for word in post.split():\n",
    "        if word not in lexicon:\n",
    "            continue\n",
    "        root_word = stemmer.stem(word)\n",
    "\n",
    "        if root_word not in words_dict:\n",
    "            words_dict[root_word] = word_val\n",
    "            word_val += 1\n",
    "\n",
    "        processed_post += \" \"+ root_word\n",
    "\n",
    "    processed_post = processed_post.strip()\n",
    "    return processed_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-burton",
   "metadata": {},
   "source": [
    "## 2.1 - Preprocessing mbti dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "compact-divide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(381453, 2) 106.30440447446999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open mbti file\n",
    "resource_location = \"../data/mbti_1.csv\"\n",
    "file = open(resource_location, 'r')\n",
    "lines = file.readlines()[1:]\n",
    "\n",
    "mean_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    personality_type, _, posts = line.partition(\",\")\n",
    "    \n",
    "    if personality_type not in personality_type_dict:\n",
    "        personality_type_dict[personality_type] = personality_type_val\n",
    "        personality_type_val += 1\n",
    "    \n",
    "    for post in posts.split(\"|||\"):\n",
    "        # Removing URLs, punctuation, and digits\n",
    "        post = post_clean_up(post)\n",
    "        \n",
    "        # Filter out posts with length less than 10\n",
    "        if len(post) > 10:\n",
    "            \n",
    "            # Apply Snowball stemmer\n",
    "            post = apply_snow_ball_stemmer(post)\n",
    "            \n",
    "            \n",
    "            # Filter out posts with length less than 10\n",
    "            if len(post) > 10:\n",
    "                mean_length += len(post)\n",
    "                example = [post, personality_type]\n",
    "                data.append(example)\n",
    "\n",
    "        \n",
    "            \n",
    "data = np.asarray(data)\n",
    "print(data.shape, mean_length/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exterior-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "processed_mbti_dict_resource_location = \"../data/processed_mbti.csv\"\n",
    "words_dict_resource_location = '../data/words_dict.json'\n",
    "personality_type_dict_resource_location = '../data/personality_dict.json'\n",
    "\n",
    "\n",
    "def save_csv(data, resource_location):\n",
    "    np.savetxt(resource_location, data, fmt='%s, %s')\n",
    "\n",
    "\n",
    "def save_json(data, resource_location):\n",
    "    with open(resource_location, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "        fp.close()\n",
    "\n",
    "# Save processed mbti data\n",
    "save_csv(data, processed_mbti_dict_resource_location)\n",
    "# Save words_dict\n",
    "save_json(words_dict, words_dict_resource_location)\n",
    "# Save words_dict\n",
    "save_json(personality_type_dict, personality_type_dict_resource_location)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
